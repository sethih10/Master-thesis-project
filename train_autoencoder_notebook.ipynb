{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders as Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Importing Autoencoder\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "The videos are contained in .h5 format. The frames can be accessed in 'frame_data' group. While time can accessed in 'time' group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'infrared-pulses/Infrared_RE/KLDT-E5WC_95775.h5'\n",
    "file = h5py.File(file_path,'r')\n",
    "dset = file['frame_data']\n",
    "video = np.zeros(dset.shape,dtype =  dset.dtype)\n",
    "video = dset\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "The usual expectation for the shape of the video is (n,176,120) where n is number of frames. \n",
    "In case the video does not satisfy the requirement, the video is reshaped into the respective using padding of 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((video.shape[0], 176,120))\n",
    "temp[:, : video.shape[1], :video.shape[1]] = video\n",
    "video = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(video):\n",
    "    \"\"\"\n",
    "    Normalizes each frame in a video.\n",
    "\n",
    "    This function takes a video represented as a NumPy array and normalizes each frame \n",
    "    to have values between 0 and 1. Normalization is performed on a per-frame basis, \n",
    "    meaning that each frame is normalized independently.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    video : numpy.ndarray\n",
    "        A 3D NumPy a63rray representing the video, where the first dimension is the \n",
    "        frame index, and the second and third dimensions are the height and width \n",
    "        of the frames, respectively.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        A 3D NumPy array of the same shape as the input, where each frame has been \n",
    "        normalized to the range [0, 1].\n",
    "\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    temp = np.empty_like(video, dtype=np.float64)  # Use float64 for more precise calculations\n",
    "    for i in range(len(video)):\n",
    "        frame = video[i]\n",
    "        temp[i] = (frame - np.min(frame)) / (np.max(frame) - np.min(frame))\n",
    "    \n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_inv = np.zeros(video.shape)  # Initialize an array for inverted video\n",
    "video_norm = normalize(video)  # Normalize the video\n",
    "\n",
    "# Invert each frame\n",
    "for i in range(len(video)):\n",
    "    video_inv[i] = 1 - video_norm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing frame index i \n",
    "i = 600\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(video[i])\n",
    "plt.title(\"Original Video\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(video_inv[i])\n",
    "plt.title(\"Inverted Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for handling image data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    images : numpy.ndarray or list\n",
    "        An array or list of images. Each image should be in a format compatible with the transforms applied\n",
    "        (e.g., 2D array for grayscale images or 3D array for RGB images).\n",
    "    transform : callable, optional\n",
    "        A function/transform to apply to each image. Default is None (no transform).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    __len__():\n",
    "        Returns the number of images in the dataset.\n",
    "    __getitem__(idx):\n",
    "        Returns the image at the specified index after applying the transform (if any).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3  # Percentage of the data to allocate to the test set\n",
    "video_train, video_test = train_test_split(video_inv, test_size=test_size)\n",
    "\n",
    "# Print shapes to verify the split\n",
    "print(f\"Training set shape: {video_train.shape}\")\n",
    "print(f\"Testing set shape: {video_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 32\n",
    "\n",
    "# Define transformations (converts to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create CustomDataset instances\n",
    "train_dataset = CustomDataset(video_train, transform=transform)\n",
    "test_dataset = CustomDataset(video_test, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "inputs = video.shape[1] * video.shape[2]  # Assuming video has shape (frames, height, width)\n",
    "layer = 2\n",
    "channels = 256\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-8\n",
    "\n",
    "# Initialize model\n",
    "model = Autoencoder(c=channels, layer=layer)\n",
    "model.to(device)  # Move model to selected device\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory name based on model configuration\n",
    "directory = f'autoencoder_{layer}l_{channels}c'\n",
    "\n",
    "# Check if directory exists; if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# Define epochs and lists to store losses\n",
    "epochs = 150\n",
    "epoch_losses = []\n",
    "epoch_val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{epochs}]')\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    train_progress = tqdm(enumerate(dataloader), total=len(dataloader), desc='Training')\n",
    "    for i, image in train_progress:\n",
    "        image = image.to(device).float()  # Move image to device and convert to float tensor\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "        output = model(image).reshape(image.shape)  # Forward pass\n",
    "        loss = loss_function(output, image)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        losses.append(loss.item())  # Record the loss\n",
    "        train_progress.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_losses = []\n",
    "    val_progress = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Validation')\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for i, image in val_progress:\n",
    "            image = image.to(device).float()  # Move image to device and convert to float tensor\n",
    "            output = model(image).reshape(image.shape)  # Forward pass\n",
    "            val_loss = loss_function(output, image)  # Compute loss\n",
    "            \n",
    "            val_losses.append(val_loss.item())  # Record the validation loss\n",
    "            val_progress.set_postfix({'val_loss': val_loss.item()})\n",
    "    \n",
    "    # Compute average losses for the epoch\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    epoch_val_loss = sum(val_losses) / len(val_losses)\n",
    "    \n",
    "    epoch_losses.append(epoch_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Train Loss: {epoch_loss:.4f} | Val Loss: {epoch_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.plot(epoch_losses, color='r', label='Training Loss')\n",
    "plt.plot(epoch_val_losses, color='b', label='Validation Loss')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Save plot as image file\n",
    "plt.savefig(os.path.join(directory, 'loss_plot.png'))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data to save to a JSON file\n",
    "data = {\n",
    "    'layers': layer,\n",
    "    'channels': channels,\n",
    "    'learning rate': lr,\n",
    "    'epoch': epochs,\n",
    "    'loss': 'mse',\n",
    "    'test size': test_size,\n",
    "    'batch size': batch_size,\n",
    "    'weight decay': weight_decay\n",
    "}\n",
    "\n",
    "# Specify the file path\n",
    "file_path = os.path.join(directory, \"parameters.json\")\n",
    "\n",
    "# Save data to the JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file)\n",
    "\n",
    "print(\"File saved successfully at:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for saving the model weights\n",
    "weights_path = os.path.join(directory,\"model_weights.pth\")\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for saving the entire model\n",
    "model_path = os.path.join(directory,\"entire_model.pth\")\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to CPU for inference\n",
    "model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select frame index 600 from 'video_inv'\n",
    "i = 600\n",
    "original = video_inv[600]\n",
    "\n",
    "# Reshape the frame to match the expected input shape of the model (assuming grayscale)\n",
    "image = original.reshape(1, 1, video.shape[1], video.shape[2])\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor of type float\n",
    "image = torch.tensor(image).float()\n",
    "\n",
    "# Perform inference using the model\n",
    "with torch.no_grad():  # Context manager to disable gradient calculation\n",
    "    output = model(image)  # Pass the input tensor through the model to get the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the output to match the dimensions of the original frame\n",
    "output = output.reshape(video.shape[1], video.shape[2])\n",
    "\n",
    "# Now 'output' contains the reconstructed frame reshaped to match 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure()\n",
    "\n",
    "# Subplot 1: Original Frame\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original)\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Subplot 2: Reconstructed Frame (Output)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output)\n",
    "plt.title(\"Filtered Image\")\n",
    "\n",
    "# Add a title for the entire figure\n",
    "plt.suptitle(\"Original vs Filtered\")\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference between the reconstructed frame ('output') and the original frame ('original')\n",
    "temp = output - original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=[10, 5])\n",
    "\n",
    "# Subplot 1: Original Frame\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original)\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Subplot 2: Reconstructed Frame (Output)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(output)\n",
    "plt.title(\"Filtered Image\")\n",
    "\n",
    "# Subplot 3: Difference between Reconstructed and Original Frames (Temp)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(temp)\n",
    "plt.title(\"Filtered - Original\")\n",
    "\n",
    "# Add a title to the entire figure\n",
    "plt.suptitle(f'Autoencoder({layer} layers) with {channels} channels for N2V files')\n",
    "\n",
    "# Save the figure to a file (assuming 'directory' is defined earlier)\n",
    "plt.savefig(os.path.join(directory, 'fig.png'))\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "# Define the pulse number and file path\n",
    "pulse_number = 95775\n",
    "file_path = f'infrared-pulses/Infrared_RE/KLDT-E5WC_{pulse_number}.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "data = h5py.File(file_path, 'r')\n",
    "\n",
    "# Access the dataset named 'frame_data' within the HDF5 file\n",
    "dset = data['frame_data']\n",
    "\n",
    "# Create a NumPy array 'video_check' with the same shape as 'dset' and copy the data\n",
    "video_check = np.zeros(dset.shape)\n",
    "video_check[:, :, :] = dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary array filled with zeros of shape (number of frames, 176, 120)\n",
    "temp = np.zeros((video_check.shape[0], 176, 120))\n",
    "\n",
    "# Copy the contents of 'video_check' into 'temp', maintaining the original data where possible\n",
    "temp[:, :video_check.shape[1], :video_check.shape[2]] = video_check\n",
    "\n",
    "# Assign the padded array 'temp' back to 'video_check'\n",
    "video_check = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from autoencoder_model import Autoencoder  # Assuming Autoencoder class is defined in autoencoder_model.py\n",
    "\n",
    "# Define the path to the model weights checkpoint\n",
    "model_path = 'results/autoencoder_3l_128c/model_weights.pth'\n",
    "\n",
    "# Define the number of channels and number of layers for the Autoencoder\n",
    "channels = 128\n",
    "layer = 3\n",
    "\n",
    "# Instantiate the Autoencoder model with the specified number of channels and layers\n",
    "model = Autoencoder(c=channels, layer=layer)\n",
    "\n",
    "# Load the model weights from the checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))  # Load checkpoint onto CPU\n",
    "model.load_state_dict(checkpoint)  # Load model weights into the model instance\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the CPU (if it's not already there)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pulse number\n",
    "pulse_number = 95775\n",
    "\n",
    "# Define the output folder name for saving video as sequence of images for tracking\n",
    "output_folder = f'Segmented Images_{pulse_number}'\n",
    "\n",
    "# Check if the output folder exists; if not, create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation pipeline for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert input data to PyTorch tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the entire video and initialize arrays for inverted and processed videos\n",
    "normalized_video = normalize(video_check)\n",
    "inverted_video = np.zeros(video_check.shape)\n",
    "filtered_video = np.zeros(video_check.shape)\n",
    "processed_video = np.zeros(video_check.shape)\n",
    "temp_video = np.zeros(video_check.shape)\n",
    "\n",
    "num_frames = len(video_check)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frame_idx in range(590, 610):\n",
    "        # Invert the normalized frame and store in inverted_video\n",
    "        inverted_video[frame_idx] = 1 - normalized_video[frame_idx]\n",
    "        \n",
    "        # Apply the transform to the inverted frame and convert to PyTorch tensor\n",
    "        transformed_frame = transform(inverted_video[frame_idx]).float()\n",
    "        inverted_tensor = transformed_frame.reshape(1, 1, inverted_video.shape[1], inverted_video.shape[2])\n",
    "        \n",
    "        # Apply the model to the inverted tensor\n",
    "        filtered_tensor = model(inverted_tensor)\n",
    "        \n",
    "        # Normalize the filtered tensor to the range [0, 1]\n",
    "        filtered_tensor_normalized = (filtered_tensor - torch.min(filtered_tensor)) / (torch.max(filtered_tensor) - torch.min(filtered_tensor))\n",
    "        filtered_video[frame_idx] = filtered_tensor_normalized.cpu().numpy().reshape(video_check.shape[1], video_check.shape[2])\n",
    "        \n",
    "    \n",
    "        # Reshape the difference tensor and store in processed_video\n",
    "        processed_video[frame_idx] = filtered_video[frame_idx] - inverted_video[frame_idx]\n",
    "        \n",
    "        # Normalize the processed frame\n",
    "        temp_frame = processed_video[frame_idx]\n",
    "        temp_frame = (temp_frame - np.min(temp_frame)) / (np.max(temp_frame) - np.min(temp_frame))\n",
    "        temp_video[frame_idx] = temp_frame\n",
    "        \n",
    "        # Save the processed frame as a TIFF file (commented out, assuming you handle saving elsewhere)\n",
    "        frame_path = os.path.join(output_folder, f\"{pulse_number}_img_{frame_idx:04d}.tif\")\n",
    "        # Image.fromarray(processed_video[frame_idx]).save(frame_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original, inverted, and processed frames\n",
    "i = 605\n",
    "plt.figure(figsize=[10, 10])\n",
    "\n",
    "# Plot the original frame\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(video_check[i])\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Plot the inverted frame\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(inverted_video[i])\n",
    "plt.title(\"Inverted\")\n",
    "\n",
    "# Plot the inverted frame\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(filtered_video[i])\n",
    "plt.title(\"Filtered\")\n",
    "\n",
    "# Plot the processed frame\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(processed_video[i])\n",
    "plt.title(\"Segmented Image\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For visualization purpose, sequence of frames saved in .h5 file are converted into a video/animation saved in .mp4 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import ArtistAnimation, writers\n",
    "import numpy as np\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "im = []\n",
    "\n",
    "for frame_id,frame in enumerate(video_save):\n",
    "    plot1 = ax1.imshow(video_check[frame_id])\n",
    "    plot2 = ax2.imshow(video_save[frame_id])\n",
    "    \n",
    "    im.append([plot1,plot2])\n",
    "    \n",
    "animation = ArtistAnimation(fig=fig,artists=im, repeat=False, interval = 50)\n",
    "plt.draw()\n",
    "plt.show()\n",
    "animation.save('autoencoder_segmenter_trained_{pulse_number}.mp4', writer=writers['ffmpeg'](fps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
